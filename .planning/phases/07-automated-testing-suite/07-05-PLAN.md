---
phase: 07-automated-testing-suite
plan: 05
type: execute
wave: 2
depends_on: ["07-01", "07-03"]
files_modified:
  - tests/e2e/notebooklm-upload.test.ts
  - tests/e2e/error-resolution.test.ts
  - tests/e2e/helpers/test-fixtures.ts
autonomous: true

must_haves:
  truths:
    - "E2E upload test completes full NotebookLM workflow"
    - "E2E error-resolution test validates feedback injection"
  artifacts:
    - path: "tests/e2e/notebooklm-upload.test.ts"
      provides: "E2E NotebookLM upload workflow test"
      min_lines: 80
    - path: "tests/e2e/error-resolution.test.ts"
      provides: "E2E error-to-resolution pipeline test"
      min_lines: 70
    - path: "tests/e2e/helpers/test-fixtures.ts"
      provides: "Shared test data fixtures"
      exports: ["VALID_CONFIG", "SAMPLE_ERROR", "MOCK_RESPONSE"]
  key_links:
    - from: "tests/e2e/notebooklm-upload.test.ts"
      to: "tests/e2e/helpers/spawn-server.js"
      via: "createTestClient"
      pattern: "createTestClient"
    - from: "tests/e2e/error-resolution.test.ts"
      to: "tests/helpers/mock-notebooklm.ts"
      via: "startMockNotebookLM"
      pattern: "startMockNotebookLM"
---

<objective>
Create E2E tests validating complete user workflows (NotebookLM upload, error-to-resolution pipeline) with spawned MCP server and mock NotebookLM UI.

**Purpose:** Validate end-to-end integration of all components (browser, MCP tools, execution engines) in realistic scenarios that users will encounter. E2E tests catch integration issues that unit and integration tests miss.

**Output:** Two E2E test suites covering full NotebookLM research workflow and error-to-resolution feedback loop with shared test fixtures.
</objective>

<execution_context>
@C:\Users\lucid\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\lucid\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-automated-testing-suite/07-RESEARCH.md
@.planning/phases/07-automated-testing-suite/07-01-SUMMARY.md
@.planning/phases/07-automated-testing-suite/07-03-SUMMARY.md

# Existing E2E infrastructure
@tests/e2e/mcp-client.test.ts
@tests/e2e/helpers/spawn-server.js
@tests/helpers/mock-notebooklm.ts

# MCP tools
@src/mcp/tools/msw-init.ts
@src/mcp/tools/msw-research.ts
@src/mcp/tools/msw-upload-sources.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test fixtures</name>
  <files>tests/e2e/helpers/test-fixtures.ts</files>
  <action>
Create `tests/e2e/helpers/test-fixtures.ts`:

```typescript
export const VALID_CONFIG = {
  version: "0.1.0",
  notebookUrl: "https://notebooklm.google.com/notebook/test-123",
  relevanceThreshold: 30,
  maxIterations: 10,
};

export const SAMPLE_ERROR = {
  type: "TypeError",
  message: "Cannot read property 'data' of undefined",
  stack: `TypeError: Cannot read property 'data' of undefined
    at processResponse (src/api/handler.ts:42:15)
    at async executeTask (src/execution/runner.ts:87:20)`,
  context: {
    file: "src/api/handler.ts",
    line: 42,
    function: "processResponse",
  },
};

export const MOCK_NOTEBOOKLM_RESPONSE = {
  answer: "This error occurs when the API response is null or undefined. Add null checking before accessing response.data property.",
  citations: ["error-handling.md", "api-patterns.md"],
  suggestedTopics: [
    "Learn about defensive programming",
    "Learn about TypeScript null safety",
  ],
};

export const SAMPLE_SOURCES = [
  {
    filename: "test-doc.md",
    content: `# Test Documentation

## Authentication
MSW uses persistent Chrome profiles to maintain Google session cookies.

## Error Handling
All API calls should include proper null checking and error boundaries.
`,
  },
  {
    filename: "api-guide.md",
    content: `# API Guide

## Best Practices
1. Always validate response before accessing properties
2. Use TypeScript strict mode for null safety
3. Implement retry logic for transient failures
`,
  },
];

export const EXPECTED_RESEARCH_OUTPUT = {
  topics: ["authentication", "error-handling", "api-patterns"],
  qaPairs: [
    {
      question: "How does authentication work?",
      answer: expect.stringContaining("Chrome profiles"),
      citations: expect.arrayContaining(["test-doc.md"]),
    },
  ],
};
```

**Fixtures provided:**
- Valid config for project initialization
- Sample coding error with stack trace
- Mock NotebookLM response
- Sample source documents
- Expected research output patterns
  </action>
  <verify>
```bash
cat tests/e2e/helpers/test-fixtures.ts | grep -c "export const"
```
Should find multiple exported constants.
  </verify>
  <done>Test fixtures exist with sample configs, errors, responses, and sources for E2E tests</done>
</task>

<task type="auto">
  <name>Task 2: Create NotebookLM upload E2E test</name>
  <files>tests/e2e/notebooklm-upload.test.ts</files>
  <action>
Create `tests/e2e/notebooklm-upload.test.ts`:

```typescript
import { describe, it, expect, beforeAll, afterAll } from "vitest";
import { createTestClient, type TestClient } from "./helpers/spawn-server.js";
import { startMockNotebookLM, type MockNotebookLMServer } from "../helpers/mock-notebooklm.js";
import { VALID_CONFIG, SAMPLE_SOURCES } from "./helpers/test-fixtures.js";
import { createTestDir, cleanupTestDir } from "../setup.js";
import fs from "node:fs";
import path from "node:path";

describe("NotebookLM upload E2E", () => {
  let tc: TestClient;
  let mockServer: MockNotebookLMServer;
  let projectDir: string;

  beforeAll(async () => {
    // Start MCP server and mock NotebookLM
    tc = await createTestClient();
    mockServer = await startMockNotebookLM();
    projectDir = createTestDir("e2e-upload");

    // Initialize MSW project
    const initResult = await tc.client.callTool({
      name: "msw_init",
      arguments: {
        projectDir,
        notebookUrl: mockServer.url,
      },
    });

    const initData = JSON.parse(initResult.content[0].text);
    expect(initData.success).toBe(true);
  }, 30000); // 30s timeout for E2E setup

  afterAll(async () => {
    await tc?.cleanup();
    await mockServer?.close();
    cleanupTestDir(projectDir);
  });

  it("uploads sources to NotebookLM", async () => {
    // Write test source files
    const sourcesDir = path.join(projectDir, "sources");
    fs.mkdirSync(sourcesDir, { recursive: true });

    for (const source of SAMPLE_SOURCES) {
      const filePath = path.join(sourcesDir, source.filename);
      fs.writeFileSync(filePath, source.content, "utf-8");
    }

    // Upload via MCP tool
    const uploadResult = await tc.client.callTool({
      name: "msw_upload_sources",
      arguments: {
        projectDir,
        sources: SAMPLE_SOURCES.map((s) => path.join(sourcesDir, s.filename)),
      },
    });

    const uploadData = JSON.parse(uploadResult.content[0].text);
    expect(uploadData.success).toBe(true);
    expect(uploadData.uploaded).toHaveLength(SAMPLE_SOURCES.length);
    expect(uploadData.uploaded).toContain("test-doc.md");
    expect(uploadData.uploaded).toContain("api-guide.md");
  });

  it("triggers research and extracts Q&A", async () => {
    // Trigger auto-conversation via MCP tool
    const researchResult = await tc.client.callTool({
      name: "msw_research",
      arguments: {
        projectDir,
        maxDepth: 2,
      },
    });

    const researchData = JSON.parse(researchResult.content[0].text);

    // Check for job ID (async operation)
    if (researchData.jobId) {
      expect(researchData.jobId).toBeTruthy();
      expect(researchData.status).toMatch(/running|queued/);

      // Poll for completion (simplified for test)
      let statusResult;
      let attempts = 0;
      while (attempts < 10) {
        statusResult = await tc.client.callTool({
          name: "msw_status",
          arguments: { projectDir },
        });

        const statusData = JSON.parse(statusResult.content[0].text);
        if (statusData.jobs?.find((j: any) => j.id === researchData.jobId && j.status === "complete")) {
          break;
        }

        await new Promise((resolve) => setTimeout(resolve, 500));
        attempts++;
      }
    }

    // Verify research output created
    const researchDir = path.join(projectDir, ".msw", "research");
    expect(fs.existsSync(researchDir)).toBe(true);

    // Check for generated markdown
    const files = fs.readdirSync(researchDir).filter(f => f.endsWith(".md"));
    expect(files.length).toBeGreaterThan(0);

    // Read and validate research content
    const reportPath = path.join(researchDir, files[0]);
    const reportContent = fs.readFileSync(reportPath, "utf-8");

    expect(reportContent).toContain("Q:");
    expect(reportContent).toContain("A:");
    expect(reportContent).toContain("Source:");
  });

  it("commits research to git", async () => {
    // Initialize git repo if not exists
    if (!fs.existsSync(path.join(projectDir, ".git"))) {
      await tc.client.callTool({
        name: "msw_init",
        arguments: {
          projectDir,
          initGit: true,
        },
      });
    }

    // Verify .msw/research/ committed
    const researchDir = path.join(projectDir, ".msw", "research");
    const gitDir = path.join(projectDir, ".git");

    expect(fs.existsSync(gitDir)).toBe(true);
    expect(fs.existsSync(researchDir)).toBe(true);

    // Check git log for research commits (simplified)
    const files = fs.readdirSync(researchDir).filter(f => f.endsWith(".md"));
    expect(files.length).toBeGreaterThan(0);
  });

  it("handles duplicate upload gracefully", async () => {
    const sourcesDir = path.join(projectDir, "sources");

    // Upload same sources again
    const uploadResult = await tc.client.callTool({
      name: "msw_upload_sources",
      arguments: {
        projectDir,
        sources: SAMPLE_SOURCES.map((s) => path.join(sourcesDir, s.filename)),
      },
    });

    const uploadData = JSON.parse(uploadResult.content[0].text);

    // Should succeed but indicate duplicates skipped
    expect(uploadData.success).toBe(true);
    expect(uploadData.skipped || uploadData.uploaded).toBeDefined();
  });
});
```

**Test coverage:**
- Upload sources to NotebookLM via MCP tool
- Trigger research and poll for completion
- Validate research output (Q&A markdown)
- Git commit of research
- Duplicate upload handling
  </action>
  <verify>
```bash
npm run test -- tests/e2e/notebooklm-upload.test.ts
```
E2E upload test passes against mock server.
  </verify>
  <done>NotebookLM upload E2E test validates complete workflow from upload to research extraction to git commit</done>
</task>

<task type="auto">
  <name>Task 3: Create error-resolution E2E test</name>
  <files>tests/e2e/error-resolution.test.ts</files>
  <action>
Create `tests/e2e/error-resolution.test.ts`:

```typescript
import { describe, it, expect, beforeAll, afterAll } from "vitest";
import { createTestClient, type TestClient } from "./helpers/spawn-server.js";
import { startMockNotebookLM, type MockNotebookLMServer } from "../helpers/mock-notebooklm.js";
import { SAMPLE_ERROR, MOCK_NOTEBOOKLM_RESPONSE } from "./helpers/test-fixtures.js";
import { createTestDir, cleanupTestDir } from "../setup.js";
import fs from "node:fs";
import path from "node:path";

describe("Error-to-resolution pipeline E2E", () => {
  let tc: TestClient;
  let mockServer: MockNotebookLMServer;
  let projectDir: string;

  beforeAll(async () => {
    tc = await createTestClient();
    mockServer = await startMockNotebookLM();
    projectDir = createTestDir("e2e-error-resolution");

    // Initialize project
    await tc.client.callTool({
      name: "msw_init",
      arguments: {
        projectDir,
        notebookUrl: mockServer.url,
      },
    });
  }, 30000);

  afterAll(async () => {
    await tc?.cleanup();
    await mockServer?.close();
    cleanupTestDir(projectDir);
  });

  it("detects error and queries NotebookLM", async () => {
    // Simulate coding agent error
    const errorLogPath = path.join(projectDir, ".msw", "errors", "latest.json");
    fs.mkdirSync(path.dirname(errorLogPath), { recursive: true });
    fs.writeFileSync(errorLogPath, JSON.stringify(SAMPLE_ERROR), "utf-8");

    // Trigger error analysis
    const analysisResult = await tc.client.callTool({
      name: "msw_research",
      arguments: {
        projectDir,
        errorContext: SAMPLE_ERROR,
      },
    });

    const analysisData = JSON.parse(analysisResult.content[0].text);

    // Should generate NotebookLM query from error
    expect(analysisData.query || analysisData.jobId).toBeDefined();
  });

  it("extracts guidance and injects into next iteration", async () => {
    // Simulate research completing with guidance
    const guidancePath = path.join(projectDir, ".msw", "guidance", "error-fix.md");
    fs.mkdirSync(path.dirname(guidancePath), { recursive: true });

    const guidanceContent = `# Fix for TypeError

${MOCK_NOTEBOOKLM_RESPONSE.answer}

## Sources
${MOCK_NOTEBOOKLM_RESPONSE.citations.map(c => `- ${c}`).join('\n')}
`;

    fs.writeFileSync(guidancePath, guidanceContent, "utf-8");

    // Execute next iteration with injected guidance
    const executeResult = await tc.client.callTool({
      name: "msw_execute",
      arguments: {
        projectDir,
        task: "fix-api-handler",
        guidance: guidanceContent,
      },
    });

    const executeData = JSON.parse(executeResult.content[0].text);

    // Should acknowledge guidance injection
    expect(executeData.guidanceInjected || executeData.success).toBeDefined();
  });

  it("validates fix and marks iteration complete", async () => {
    // Simulate fix applied
    const fixedFilePath = path.join(projectDir, "src", "api", "handler.ts");
    fs.mkdirSync(path.dirname(fixedFilePath), { recursive: true });

    const fixedCode = `
export function processResponse(response: ApiResponse | null) {
  // Fixed: Added null check per NotebookLM guidance
  if (!response || !response.data) {
    throw new Error("Invalid API response");
  }
  return response.data;
}
`;

    fs.writeFileSync(fixedFilePath, fixedCode, "utf-8");

    // Verify fix via MCP tool
    const verifyResult = await tc.client.callTool({
      name: "msw_verify",
      arguments: {
        projectDir,
        target: fixedFilePath,
      },
    });

    const verifyData = JSON.parse(verifyResult.content[0].text);

    // Should confirm null check added
    expect(verifyData.passed || verifyData.success).toBeDefined();
  });

  it("handles multiple errors in sequence", async () => {
    const errors = [
      { ...SAMPLE_ERROR, message: "Error 1" },
      { ...SAMPLE_ERROR, message: "Error 2" },
      { ...SAMPLE_ERROR, message: "Error 3" },
    ];

    for (const error of errors) {
      const errorPath = path.join(projectDir, ".msw", "errors", `${Date.now()}.json`);
      fs.writeFileSync(errorPath, JSON.stringify(error), "utf-8");

      // Trigger analysis
      await tc.client.callTool({
        name: "msw_research",
        arguments: {
          projectDir,
          errorContext: error,
        },
      });
    }

    // Verify all errors logged
    const errorsDir = path.join(projectDir, ".msw", "errors");
    const errorFiles = fs.readdirSync(errorsDir).filter(f => f.endsWith(".json"));

    expect(errorFiles.length).toBeGreaterThanOrEqual(errors.length);
  });

  it("prevents infinite loop with max iterations", async () => {
    // Set low max iterations
    const configPath = path.join(projectDir, ".msw", "config.json");
    const config = JSON.parse(fs.readFileSync(configPath, "utf-8"));
    config.maxIterations = 3;
    fs.writeFileSync(configPath, JSON.stringify(config), "utf-8");

    // Simulate repeated failures
    for (let i = 0; i < 5; i++) {
      const result = await tc.client.callTool({
        name: "msw_execute",
        arguments: {
          projectDir,
          task: `failing-task-${i}`,
        },
      });

      const data = JSON.parse(result.content[0].text);

      // After 3 iterations, should stop
      if (i >= 3) {
        expect(data.stopped || data.maxIterationsReached).toBeDefined();
      }
    }
  });
});
```

**Test coverage:**
- Error detection and NotebookLM query generation
- Guidance extraction and injection into next iteration
- Fix validation
- Multiple errors handled in sequence
- Max iterations prevents infinite loops
  </action>
  <verify>
```bash
npm run test -- tests/e2e/error-resolution.test.ts
npm run test:coverage -- tests/e2e/
```
E2E error-resolution test passes, E2E coverage at 70%+.
  </verify>
  <done>Error-resolution E2E test validates complete feedback loop from error detection to guidance injection to fix validation</done>
</task>

</tasks>

<verification>
**E2E test checks:**
1. Run `npm run test -- tests/e2e/` - all E2E tests pass
2. Run `npm run test:coverage -- tests/e2e/` - E2E coverage at 70%+
3. Verify E2E tests spawn real MCP server via stdio
4. Check E2E test execution time (< 2 minutes for all E2E tests)

**Workflow validation:**
- Upload workflow: sources -> research -> Q&A extraction -> git commit
- Error-resolution workflow: error -> query -> guidance -> fix -> verify
</verification>

<success_criteria>
**E2E tests are complete when:**
1. NotebookLM upload test validates complete workflow (upload, research, extraction, commit)
2. Error-resolution test validates feedback loop (error, query, guidance, fix, verify)
3. E2E tests use spawned MCP server and mock NotebookLM UI
4. All E2E tests pass consistently (no flakiness)
5. Test fixtures provide reusable sample data
6. E2E test suite runs in < 2 minutes
7. E2E coverage at 70%+ on critical paths
</success_criteria>

<output>
After completion, create `.planning/phases/07-automated-testing-suite/07-05-SUMMARY.md` with:
- E2E test suites created (notebooklm-upload, error-resolution)
- Workflows validated (upload flow, error-resolution flow)
- Test fixtures created (configs, errors, responses, sources)
- E2E coverage achieved (% across MCP tools, execution engines)
- Test execution time (target: < 2 min)
- Next steps: Snapshot testing and coverage validation (07-06)
</output>
