### Research Report: Architectural Blueprint for the MSW Protocol This document provides a detailed, implementation-ready analysis for constructing the MSW (Make Shit Work) Protocol, an autonomous coding system integrating GSD Protocol, NotebookLM MCP, and the Ralph Wiggum Loop. All guidance is derived exclusively from the provided source materials. \-------------------------------------------------------------------------------- \#\#\#\#\# 1.0 NotebookLM Integration and Automation Strategy The strategic goal of integrating NotebookLM is to provide the AI agent with a zero-hallucination, research-grounded knowledge base. This section defines the required technical patterns to establish a robust, non-brittle, and automated connection between the MSW Protocol and the NotebookLM service. These API-driven patterns are mandated to explicitly reject unstable alternatives like UI scraping for all core operations. \#\#\#\#\#\# 1.1 Persistent Session and Authentication Management The various NotebookLM MCP and CLI implementations all rely on a consistent pattern for establishing persistent, authenticated sessions suitable for automation. This architecture is the foundation for any autonomous agent interacting with NotebookLM and is achieved through a three-step process: 1\. **One-Time Interactive Login:** The initial setup requires a single interactive command, such as nlm login or uv run notebooklm-mcp init. This command launches a dedicated Chrome browser instance, allowing a human user to perform a manual Google login. 2\. **Session Artifact Storage:** Upon successful authentication, the system extracts the necessary authentication cookies and session data. These artifacts are stored locally in a dedicated profile directory (e.g., chrome\_profile\_notebooklm/ or within the \~/.notebooklm-mcp-cli directory). 3\. **Headless Operation:** All subsequent executions of the MCP server or CLI can operate in a fully headless mode. The system reuses the stored session artifacts to maintain authentication across multiple runs and reboots, eliminating the need for further user interaction. For debugging or headless setup, standalone utilities like notebooklm-mcp-auth are also available. This architecture is fundamental to the MSW Protocol, as it enables the autonomous agent to maintain its connection to the knowledge base without requiring manual intervention. \#\#\#\#\#\# 1.2 Topic and Information Extraction A critical analysis of the available source material indicates that any directive to interact with UI elements like "topic pills" is based on a misconception of how the underlying tools function. The notebooklm-mcp and notebooklm-mcp-cli tools **shall not operate via UI browser automation** for querying. While some implementations use browser automation technology with humanization features for the initial session management, the core *querying* mechanism relies on direct, internal HTTP/RPC calls to the NotebookLM backend. This distinction is vital for architectural stability. The correct, API-driven workflow for information extraction is a two-part process: 1\. **Broad Research:** First, the system will initiate a comprehensive search using a tool like research\_start with mode='deep'. This command instructs NotebookLM to perform a thorough investigation of a topic, gather relevant sources from the web or Google Drive, and import them into a new or existing notebook. 2\. **Targeted Queries:** Once the knowledge base is populated, the system will programmatically iterate through a list of key topics or specific questions. For each item, it uses the notebook\_query tool to receive a precise, synthesized, and citation-backed answer derived exclusively from the sources within that notebook. This programmatic approach is superior to fragile UI scraping, as it is not susceptible to changes in the web interface and provides a more direct, efficient, and reliable method for grounding the agent's knowledge. \#\#\#\#\#\# 1.3 Managing Asynchronous Responses The MSW Protocol must be capable of handling both immediate and long-running operations when interacting with NotebookLM. The source documents describe three primary patterns for managing responses: \* **Request/Response Pattern:** This is the most common method. Tools like notebook\_query or send\_chat\_message are blocking calls; the client sends a request and waits for the full, synthesized response before proceeding. The khengyun/notebooklm-mcp implementation also includes a timeout parameter to manage these blocking calls effectively. \* **Polling for Long-Running Jobs:** Operations such as content generation via studio\_create return immediately with an artifact\_id. The MSW system must then use a separate status-checking tool to poll this ID periodically. The job is complete only when the status check indicates success and provides a download URL or other result. \* **Streaming (Server-Sent Events):** The khengyun/notebooklm-mcp server implementation offers support for Server-Sent Events (SSE) as an advanced option. This allows for the real-time streaming of response data as it is generated, which can be beneficial for user-facing interactive chats but is less critical for autonomous backend operations. For the MSW system, the polling pattern for long-running jobs is the most practical and recommended approach, as it prevents client timeouts and allows the agent to handle other tasks while waiting for resource-intensive generation to complete. \#\#\#\#\#\# 1.4 Rate Limiting and Mitigation Strategy The source documents explicitly state that the free tier of NotebookLM is subject to rate limits. This is a critical constraint for any system designed for continuous operation. | Constraint | Mitigation Strategy | | \------ | \------ | | **Daily Query Limit** | The free tier is limited to approximately 50 queries per day per Google account. | | **Multi-Account Rotation** | The architecture will implement support for multiple Google accounts using the notebooklm-mcp-cli's named profile feature (e.g., nlm login \--profile work). The MSW Protocol mandates a mechanism to detect a rate limit error and automatically cycle to the next available profile to continue its work. | Therefore, a multi-profile rotation mechanism is a non-negotiable, foundational requirement for the MSW Protocol's autonomous operation. \#\#\#\#\# 2.0 GSD Protocol Integration The GSD (Get Shit Done) Protocol serves as the structured, spec-driven development engine for the MSW system. Its primary strategic function is to ensure that all code generation is methodical, traceable, and subject to quality control. GSD provides the framework for breaking down complex tasks into atomic, verifiable units of work, thereby preventing the context degradation and quality drift common in long-running AI sessions. \#\#\#\#\#\# 2.1 Codebase Analysis and Orchestration Pattern The GSD Protocol's /gsd:map-codebase command is the designated entry point for working with an existing codebase. Its purpose is to conduct a thorough analysis of the project's structure, dependencies, architectural patterns, coding conventions, and primary concerns *before* any new code is generated. This command employs a sophisticated multi-agent orchestration pattern to ensure high-quality analysis without degrading the main session's context: \* A central, "thin orchestrator" is responsible for coordination. \* The orchestrator spawns multiple specialized researcher agents (e.g., four) that work in parallel. Each agent is assigned a specific area of the codebase to investigate (e.g., stack, features, architecture). \* Crucially, each researcher agent operates within a completely fresh context window. This prevents the accumulation of irrelevant information and ensures that each analysis is sharp and focused. \* The orchestrator then collects the findings from all parallel agents and synthesizes them into a comprehensive overview of the codebase. This parallel, fresh-context approach is a core principle of GSD, designed to maintain the highest possible quality of analysis and understanding, which then informs all subsequent planning and execution phases. \#\#\#\#\#\# 2.2 Deconstructing the PLAN.md XML Format The PLAN.md file is the central artifact for GSD's execution phase. It uses a structured XML format to define an atomic unit of work with unambiguous instructions and built-in verification. \* \*\*\*\* **:** The root element, representing a single, self-contained unit of work. It may include attributes like type="auto" to inform the execution engine. \* \*\*\*\* **:** A human-readable title that succinctly describes the task's objective. \* \*\*\*\* **:** Specifies the exact file or files that are to be created or modified by the task. \* \*\*\*\* **:** Contains precise, step-by-step instructions for the AI agent. This section is designed to be clear and unambiguous, leaving no room for interpretation. \* \*\*\*\* **:** Defines a concrete, executable command (e.g., a curl request, a test script) or a clear, objective check that can be used to confirm the task was completed successfully. \* \*\*\*\* **:** Describes the final state or the criteria that must be met to consider the task truly complete from a functional standpoint. During execution, the GSD system programmatically uses the command and conceptually checks against the criteria to validate the agent's work, creating a tight, traceable link between the plan and its successful implementation. \#\#\#\#\#\# 2.3 State Management Across Sessions GSD solves the critical problems of context rot and memory limitations in LLMs by persisting its state to a set of dedicated Markdown files within a .planning/ directory at the project root. This file-based mechanism acts as the system's long-term memory, allowing work to be paused and resumed seamlessly across sessions. | File | Purpose | | \------ | \------ | | STATE.md | The primary state-tracking file. It records key decisions, current blockers, and the system's exact position in the workflow. This file acts as the agent's short-term memory, enabling it to resume work instantly even after a full context clear. | | ROADMAP.md | Contains the high-level phases of the project. It tracks which major stages have been completed and what is next on the agenda, serving as the project's strategic guide. | | PROJECT.md | Stores the overall project vision, goals, and constraints. This document serves as a constant, stable reference point for all agents to ensure their work remains aligned with the project's core objectives. | This robust persistence mechanism ensures that the quality of the agent's work does not degrade over time and that crucial context is never lost between sessions. \#\#\#\#\# 3.0 Ralph Wiggum Loop Integration The Ralph Wiggum Loop functions as the resilient execution layer within the MSW Protocol. It is designed for continuous, self-correcting iteration on a single task. Its role is to repeatedly attempt an action and run a verification check, persisting until the defined success criteria are met or a maximum number of attempts is reached. This provides a robust mechanism for handling tasks that may require multiple refinements, such as fixing failing tests. \#\#\#\#\#\# 3.1 Stop Hook Interception Mechanism The core of the Ralph Wiggum loop is its use of the Claude Code stop hook mechanism, as detailed in related GitHub issues. This mechanism enables a script to take control when the model attempts to terminate a session. 1\. **Interception:** The stop-hook.sh script is registered in the Claude Code configuration to run automatically whenever the model session is about to end. 2\. **Input Processing:** The script receives a JSON object via an environment variable (HOOK\_INPUT), which contains session metadata like the path to the conversation transcript (transcript\_path). The script uses a utility like jq to parse this JSON and analyze the session's final state. 3\. **Continuation Logic:** Based on its analysis (e.g., detecting a test failure), the script constructs a new JSON payload containing the prompt for the *next* iteration. 4\. **Re-injection and Continuation:** The script prints this JSON payload to stderr and then immediately terminates with exit code 2\. This specific exit code is a signal to the Claude Code client, instructing it *not* to terminate the session but to instead take the content from stderr and use it as the next user prompt, thereby creating a loop. 5\. **State Management:** All state, such as loop counters or previous error logs, must be managed externally by the stop-hook.sh script itself, typically by reading from and writing to a local state file between invocations. The combination of stderr output followed by an exit 2 status code is the central technical driver that enables the iterative, self-perpetuating loop. \#\#\#\#\#\# 3.2 Injecting Research into the Iteration Loop By synthesizing the capabilities of the Ralph Wiggum loop and the NotebookLM CLI, the architecture will create a powerful, self-correcting workflow that is grounded in a reliable knowledge base. 1\. **Failure Detection:** The stop-hook.sh script executes the verification command (e.g., runs a test suite) and parses the transcript or command output to detect a failure. 2\. **Query Formulation:** Upon failure, the script extracts the specific error message or stack trace from the log. 3\. **Programmatic Research:** The script then invokes the notebooklm-mcp-cli tool to query a relevant, pre-populated knowledge base. For example: nlm notebook query "My Project Docs" "How to resolve error: extracted error message". 4\. **Context Injection:** The script captures the text output from the NotebookLM query. It then constructs the re-injection prompt for the next iteration, prepending the research findings to provide the model with a potential solution. For instance: *"Previous attempt failed. Research from NotebookLM suggests the following: captured output. Now, attempt to fix the issue based on this new information."* 5\. **Tracking Attempts:** To prevent infinite loops on unresolvable errors, the script shall log the *hash* of the error message and the returned NotebookLM suggestion in a local state file (e.g., .ralph\_state.json). This ensures it does not retry the same failed solution for trivially different but semantically identical error strings. This workflow directly connects the self-correcting execution loop with the zero-hallucination research base, enabling the agent to learn from its mistakes and overcome obstacles autonomously. \#\#\#\#\#\# 3.3 The Completion Mechanism The Ralph Wiggum loop is not infinite. The "completion promise" is fulfilled by the logic within the stop-hook.sh script itself. The loop terminates when the script, after running its verification step (e.g., executing the project's test suite), determines that the success criteria (equivalent to GSD's criteria) have been met. Upon successful verification, the script simply **exits with a code other than 2** (typically exit 0). This standard exit code signals to the Claude Code client that the hook's work is finished, the loop condition is satisfied, and the session can be terminated normally. \#\#\#\#\# 4.0 Unified MSW MCP Server Architecture To provide a cohesive and simplified interface for the AI agent, the MSW Protocol will be exposed through a single, unified Model Context Protocol (MCP) server. This approach centralizes all available capabilities into a single set of tools, streamlining the agent's interaction model, simplifying state management, and reducing the overhead of connecting to multiple disparate services. \#\#\#\#\#\# 4.1 Recommended Server Strategy The most pragmatic and efficient architectural strategy is to **extend the existing** **notebooklm-mcp** **server** . This approach is mandated for several key reasons: \* **Leverages Existing Foundation:** It builds upon a robust, tested server implementation that already handles the most complex aspect of the integration: persistent authentication and communication with Google's NotebookLM service. \* **Unified Toolset:** It allows GSD and Ralph Wiggum functionalities to be exposed as new, distinct tools alongside the existing NotebookLM tools. This creates a single, coherent point of interaction for the AI agent, which can discover and utilize all protocol capabilities from one connection. \* **Simplicity:** It avoids the significant architectural complexity of managing inter-server communication, building a proxy/gateway from scratch, or forcing the agent to juggle multiple MCP connections. Extending the mature notebooklm-mcp server is the only viable option for long-term maintainability and scalability, preventing architectural fragmentation. \#\#\#\#\#\# 4.2 Proposed MSW Tool Definitions The unified MSW server will expose a minimal, well-defined set of tools that encapsulate the core functionalities of the integrated protocols. **NotebookLM Research Tools (from** **jacob-bd/notebooklm-mcp-cli** **)** | Tool Name | Parameters | Description | | \------ | \------ | \------ | | notebook\_query | notebook\_id: str, query: str | Asks a question to a specific notebook and gets a synthesized, citation-backed answer. | | research\_start | topic: str, mode: str ('deep' or 'standard') | Performs a web or Drive research task on a topic, returning sources. | | source\_add | notebook\_id: str, url: str or file: path or text: str | Adds a new source to a specified notebook. | **GSD Protocol Tools (adapted from** **get-shit-done** **commands)** | Tool Name | Parameters | Description | | \------ | \------ | \------ | | gsd\_plan\_phase | phase\_number: int | Generates a detailed, multi-step execution plan (PLAN.md) for a given phase in the project roadmap. | | gsd\_execute\_phase | phase\_number: int | Executes all plans for a given phase, running tasks in waves with fresh context and creating atomic git commits. Returns a job ID. | **Ralph Wiggum Execution Tool** | Tool Name | Parameters | Description | | \------ | \------ | \------ | | execute\_and\_verify | plan\_path: str, test\_command: str, max\_iterations: int | Initiates a Ralph Wiggum loop to execute a plan, continuously running the test\_command until it passes or max\_iterations is reached. Leverages NotebookLM for research on failures. | \#\#\#\#\#\# 4.3 Handling Long-Running Operations Operations such as gsd\_execute\_phase can take a significant amount of time to complete, far exceeding typical client timeout thresholds. To handle this, the MSW server must adopt an asynchronous, polling-based pattern for all long-running jobs. 1\. **Immediate Response:** The tool initiating the long-running operation (e.g., gsd\_execute\_phase) shall return immediately with a unique job\_id. 2\. **Status Polling:** A separate, dedicated tool, such as get\_job\_status(job\_id: str), must be exposed. The AI agent can call this tool periodically to check the job's status (e.g., running, completed, failed). 3\. **Result Retrieval:** Once the status is completed, a final tool, such as get\_job\_result(job\_id: str), can be called to retrieve the final output, summary, or any artifacts generated by the job. This asynchronous pattern is essential for maintaining a responsive client-server interaction. The get\_job\_status tool should also return an estimated completion time or progress percentage if the underlying GSD process can provide it, allowing a sophisticated agent to manage its own time and expectations. \#\#\#\#\# 5.0 Research Extraction and Reporting Workflow A core capability of the MSW Protocol is its ability to perform deep, grounded research on a topic and compile the findings into a structured, committable artifact. This automated process transforms the AI agent from a simple coder into a research assistant capable of documenting its own knowledge acquisition. This section outlines the specific, programmatic workflow for achieving this. \#\#\#\#\#\# 5.1 Pseudocode for Automated Research Reporting The following pseudocode details the research and reporting workflow. This process relies entirely on the programmatic tools exposed by the notebooklm-mcp server and shall not involve any fragile UI automation.  function generateResearchReport(mainTopic): // 1\. Create a dedicated notebook for this research task notebook \= call\_mcp\_tool('notebook\_create', { 'title': \`Research on ${mainTopic}\` }) // 2\. Perform deep research to find and import sources researchResult \= call\_mcp\_tool('research\_start', { 'topic': mainTopic, 'mode': 'deep' }) for source in researchResult.sources: call\_mcp\_tool('source\_add', { 'notebook\_id': notebook.id, 'url': source.url }) // 3\. Dynamically generate key questions via meta-query reportContent \= "" metaQuery \= "Generate 5 key questions to ask about the topic: " \+ mainTopic questionResponse \= call\_mcp\_tool('notebook\_query', { 'notebook\_id': notebook.id, 'query': metaQuery }) keyQuestions \= parse\_questions\_from\_response(questionResponse.text) // 4\. Iterate through generated questions and query the notebook for question in keyQuestions: answer \= call\_mcp\_tool('notebook\_query', { 'notebook\_id': notebook.id, 'query': question }) reportContent \+= "\#\# " \+ question \+ "\\n\\n" reportContent \+= answer.text \+ "\\n\\n" // 5\. Compile the final Markdown report finalReport \= "\# Research Report: " \+ mainTopic \+ "\\n\\n" \+ reportContent // 6\. Save and commit the report save\_file("RESEARCH\_REPORT.md", finalReport) git\_commit("docs(research): Generate report on " \+ mainTopic) return "Report generated and committed successfully."  \#\#\#\#\#\# 5.2 Optimal Markdown Report Structure To ensure the generated research artifacts are professional, readable, and consistent, the system shall use a standardized Markdown template. markdown \# Research Report: \[Topic\] \- \*\*Generated:\*\* \[Timestamp\] \- \*\*Source Notebook:\*\* \[URL to NotebookLM notebook\] \--- \#\# Table of Contents \- \[Question 1\](\#question-1) \- \[Question 2\](\#question-2) \- \[Synthesis\](\#synthesis) \--- \#\# \[Question 1\] \[Synthesized answer from NotebookLM for the first query...\] \#\# \[Question 2\] \[Synthesized answer from NotebookLM for the second query...\] \--- \#\# Synthesis \[An optional final section where the agent provides a high-level summary of all findings.\]  \#\#\#\#\#\# 5.3 Git Commit Message Formatting Drawing inspiration from the clean, atomic commit messages generated by the GSD Protocol, all research-related commits shall adhere to a standardized format. This practice ensures a clean, readable, and machine-parseable Git history. \* **Format:** docs(research): on \* **Example 1:** docs(research): Extract topics on MSW Protocol from NotebookLM \* **Example 2:** docs(research): Generate initial report on autonomous coding agents \* **Rationale:** This convention clearly communicates the commit's scope (docs), sub-scope (research), the specific action performed, and the subject matter. It separates documentation and research activities from feature development, improving project observability. \#\#\#\#\# 6.0 Implementation Roadmap and Risk Assessment This final section provides a strategic plan for constructing the MSW Protocol. It defines the critical path for MVP validation, identifies which components can be leveraged from existing projects versus what must be developed, and mandates mitigation strategies for the following high-priority risks. \#\#\#\#\#\# 6.1 Minimum Viable Product (MVP) Implementation Order The following five steps represent the most direct path to building and validating the core concepts of the MSW Protocol. This order prioritizes validating each component's functionality and their basic integration points. 1\. **Establish NotebookLM CLI Access:** Install and authenticate the notebooklm-mcp-cli package. From the terminal, successfully execute a manual query (nlm notebook query ...) against a pre-populated notebook. This validates that the foundational research layer is operational. 2\. **Install and Initialize GSD:** Install the get-shit-done system and run /gsd:new-project on a test repository. Manually create and execute a simple PLAN.md file to confirm that the core spec-driven development workflow is functional. 3\. **Create the GSD-to-NotebookLM Bridge:** Write a simple shell script that accepts a string as input, passes it as a query to the nlm CLI command, and prints the standard output. This script represents the first, most basic version of the "research-grounding" link. 4\. **Integrate the Bridge into a GSD Plan:** Modify a PLAN.md file to invoke the script from the previous step within an tag. Execute the plan using /gsd:execute-phase to prove that the GSD system can successfully trigger an external research query. 5\. **Validate the Basic Iteration Loop:** Isolate the stop-hook.sh script from the Ralph Wiggum plugin. Configure it for a simple, non-coding task (e.g., a test that checks if a file contains the word "done"). Manually trigger failures to confirm that the loop correctly intercepts the exit signal and continues the session as expected. \#\#\#\#\#\# 6.2 Component Analysis: Reuse vs. New Development A clear delineation between existing, reusable components and net-new development is critical for accurate project planning. | Component to Reuse | Component to Build New | | \------ | \------ | | The complete notebooklm-mcp-cli package (as a callable tool). | The master orchestration logic (the "MSW Protocol") that decides when to call GSD, NotebookLM, or Ralph Wiggum based on task context. | | The get-shit-done command-line system and its underlying multi-agent architecture. | The custom tool definitions and wrapper logic for the unified MCP server (e.g., gsd\_execute\_phase, execute\_and\_verify). | | The core interception logic from the ralph-wiggum/stop-hook.sh script, specifically the exit 2 and stderr redirection pattern. | The intelligent logic within the stop hook to parse complex error messages, formulate relevant NotebookLM queries, and inject the resulting context into the next iteration prompt. | | | A comprehensive logging and observability layer to track agent decisions, tool calls, research queries, and iteration outcomes. This is critical for debugging and performance tuning the autonomous system. | \#\#\#\#\#\# 6.3 Major Technical Risks and Unknowns The successful implementation of the MSW Protocol is subject to several significant external risks and internal complexities that must be acknowledged. \* **Reliance on Unofficial APIs:** This is the single greatest technical risk to the project. The notebooklm-mcp and notebooklm-mcp-cli projects explicitly state they use internal, undocumented Google APIs. These APIs are subject to change, deprecation, or restriction at any time without warning, which would completely break the research and knowledge-grounding component of the MSW Protocol. \* **Strict Rate Limiting:** The free tier's daily query limit of approximately 50 queries per account is fundamentally incompatible with a truly autonomous system. A complex coding task could easily require hundreds of research queries, rendering the system unusable at scale without a robust multi-account rotation strategy, which itself adds complexity. \* **Complexity of Failure-to-Query Mapping:** The MSW Protocol must include a dedicated "Error Canonicalization" module. This module will be responsible for parsing varied error outputs (e.g., stack traces, compiler errors) into a standardized, concise natural language format *before* sending the query to NotebookLM. This is a non-trivial but mandatory component for the system's self-correction capability to function reliably. \* **Conceptual Misalignment on UI Automation:** The initial directive's mention of "clicking topic pills" suggests a potential misunderstanding of how the underlying notebooklm-mcp tools operate. Pursuing a UI automation path using a tool like dev-browser would introduce extreme fragility, as it would be susceptible to even minor changes in the NotebookLM web interface. Adhering to the mandated API-driven approach is critical to avoid this major architectural risk.

